kubectl label nodes/namespace имя ноды/пространства ключь=значение метки (создаем метки для узла чтобы можно было через нее указать что хотим разместить pod именно на узле с такой меткой)
kubectl get nodes --show-labels (что бы посмотреть метки на нодах, но можно и через describe)
kubectl describe pod/replicaset/deployment/node/... имя того, что хочешь посмотреть (после одного из четырех пишем имя, описание которого хотим посмотреть, покажет полное описание)
kubectl get pods -o wide (покажет информацию о запущенных контейнерах с дополнительными полями ip и node) kubectl get pods -w (что бы посмотреть процессы с подами на ходу)
kubectl delete pod/deployment/replicaset/quota/all --all -n пространство имен (удалить все из указанного, если не указывать all и --all, то нужно указать отдельное имя того, что удаляем)
kubectl apply -f имя ямл файла (применит конфигурацию по имени файла, то есть, после изменений в файле перечитываем конфигурацию и приложение обновляется, можно сделать на живую не удаляя)
kubectl create -f имя ямл файла (создаст pod из ямл файла с конфигурацией)
kubectl replace -f имя ямл файла (эта команда применяется тогда, когда ты внутри файла изменил кол-во реплик и мы перечитываем ямл файл)
kubectl scale --replicas=кол-во -f имя файла (этой командой мы изменяем кол-во реплик в ямл файле не заходя в него)
kubectl get replicaset (чтобы посмотреть список созданных репликасет) kubectl get replicasets (что бы посмотреть старый и новый набор реплик после обновления деплоймента)
kubectl delete replicaset/deployment/pod имя одного из них в ямл файле (удалит экземпляр одного из них, которые созданы)
kubectl get all (покажет все репликасет, деплойменты и поды)  kubectl get secrets (покажет все секреты в кластере)
kubectl get deployments (чтобы посмотреть список созданных деплойментов)
kubectl rollout status deployment/имя деплоймента (чтобы посмотреть состояние выкатки обновлений на ходу)
kubectl rollout restart deployment/имя деплоймента (передеплоить текущую версию)
kubectl rollout history deployment/имя деплоймента (для просмотра списка ревизий и истории изменений)
kubectl rollout undo deployment/имя деплоймента (что бы откатиться назад к старой версии, если что-то пошло не так в новой)
kubectl edit deploy имя деплоймента --record (откроет манифест с большим количеством параметров, которые не указывались ямл файле, из которого создан деплоймент, для глобальных изменений)
kubectl exec -it имя пода bash (зайдем в под с командной оболочкой bash)
kubectl get nodes (покажет ноды) kubectl top nodes (покажет ноды в ресурсами потребления)
kubectl delete ds имя демонсета в манифесте (удалит daemonset)
kubectl get clusterrole/role (покажет имеющиеся кластер-роли или простые роли для пространств имен, указываем что-то одно для вывода)
kubectl get quota -n название пространства имен (выведет все квоты в указанном пространстве)(через describe с указанием имени квоты и пространства можно посмотреть насколько занята квота)
kubectl get serviceaccount (покажет все сервис аккаунты в кластере)  kubectl get namespace (покажет все пространства имен в кластере)
kubectl get clusterrolebinding/rolesbinding (выведет все связки в кластере указанного типа)
kubectl get имя службы (покажет все что есть из того что укажем, можно еще пространство указать)
kubectl logs имя пода -с имя контейнера как в манифесте (покажет логи всего контейнера) kubectl logs -f имя пода (покажет логи всего пода)
kubectl logs --tail=10 имя пода (выведет указанное кол-во строк лога с конца)  kubectl logs имя пода (покажет логи пода)
kubectl api-versions (покажет версии api в кластере)   kubectl api-resources (покажет api ресурсов в кластере)
docker ps (смотрим список запущенных контейнеров, с ключом -q — «тихий» режим, в котором команда выводит только id контейнеров, -a — показывает все контейнеры, а не только запущенные)
docker pull ubuntu:14.04 (загрузка образа)  docker run --name имя для контейнера имя образа (запустить контейнер с выбранным именем из указанного образа)
docker logs id-контейнера или имя (Позволяет просмотреть логи указанного контейнера) docker top имя контейнера (посмотреть процессы контейнера)
docker stop/start/restart id-контейнера или имя (остановит/запустит/перезапустит контейнер, можно несколько, только одно действие)
docker kill id-контейнера или имя (убивает процесс с PID1 - сам контейнер, можно несколько, работает как stop только жестко, после запуска состояние востановиться)
docker info (покажет общесистемную информацию) docker stats (покажет в реальном времени использование ресурсов) docker images (покажет список всех образов, хранящихся на машине) 
docker inspect имя образа (Docker inspect отображает низкоуровневую информацию о конкретном объекте Docker) docker build . (собрать dockerfile в текущей директории)
docker volume create имя тома(создать том чтобы хранить данные) docker volume ls(посмотреть список) docker volume inspect имя тома (посмотреть том) docker volume rm имя тома (удалить том) 
docker run -v имя тома на хосте:/path/to/mount в контейнере --name имя для контейнера имя образа (смонтировать созданный том в контейнер при запуске)
docker network create -d имя-driver имя сети (создать сеть) docker network rm имя сети (удалить сеть) docker network inspect имя сети(посмотреть сеть) docker network ls(посмотреть список)
docker network connect/disconnect имя сети id-контейнера или имя(добавить/удалить контейнер в/из сети, это для того что бы контейнеры могли взаимодействовать в одном сетевом пространстве) 
docker run -d -p localhostport80:containerport80 -e "port=containerport" --name имя для контейнера --network=имя сети для подключения имя образа (запустить контейтер и подключить к сети)



ПРИМЕР КОНФИГУРАЦИИ Pod С ПЕРЕМЕННЫМИ СРЕДЫ 
apiVersion: v1 (версия api зависит от типа службы)
kind: Pod (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-nginx (имя пода, оно может быть, какое захочешь)
   labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
      app: myapp (имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  containers: (под ним создаем описание с заполнением полей для контейнеров)
    - name: container-nginx (имя контейнера, какое захочешь)
      image: redis (образ, который будет скачан из репозиториев и установлен)
      imagePullPolicy: Always(Always-Образ скачивается при каждом запуске,Never-будет использоваться образ,который раннее скачан,IfNotPresent-загрузится и будет использоваться при запуске)
      ports: (порт приложения)
        - containerPort: 8080 (указываем порт)
          protocol: TCP (протокол по которому будут передаваться данные)
      env: (пример создания перененных окружения, ниже указываем переменные по типу - имя переменной и ее значение)
        - name: SERVICE_PORT (имя перменной)
          value: "8080"  (значение)
        - name: SERVICE_IP (имя перменной)
          value: "172.17.0.1" (значение)
        - name: PROTOCOL  (имя перменной)
          value: "https" (значение)
      envFrom: (ПРИМЕР ТОГО, КАК ОПРЕДЕЛИТЬ ВСЕ ЗНАЧЕНИЯ ИЗ СЕКРЕТА КАК ПЕРЕМЕННЫЕ СРЕДЫ РАЗОМ, А НЕ ПО ОТДЕЛЬНОСТИ)
        - secretRef:  (то откуда берем) (значения из configmap определяются также только название этого поля будет configMapKeyRef)
             name: my-secret (имя секрета как в манифесте)
      env: (ПРИМЕР ТОГО, КАК ОПРЕДЕЛИТЬ ЧЕРЕЗ ПЕРЕМЕННУЮ СРЕДЫ ЗНАЧЕНИЯ ВЗЯТЫЕ ИЗ СЕКРЕТА ВНУТРЬ КОНТЕЙНЕРА, КАЖДЫЙ ПАРАМЕТР ОТДЕЛЬНО)
       - name: SECRET_USERNAME (имя перменной какое захочешь)
           valueFrom: (ниже определяем откуда берем данные)
             secretKeyRef: (то откуда берем) (значения из configmap определяются также только название этого поля будет configMapKeyRef)
                key: username (имя ключа в секрете значение которого положим в переменную) 
                name: my-secret (имя секрета как в манифесте)
       - name: SECRET_RASSWORD (имя перменной какое захочешь)
           valueFrom: (ниже определяем откуда берем данные)
             secretKeyRef: (то откуда берем) (значения из configmap определяются также только название этого поля будет configMapKeyRef)
               name: my-secret (имя секрета как в манифесте)
               key: password  (имя ключа в секрете значение которого положим в переменную)      
      volumeMounts: (ПРИМЕР ТОГО, КАК МОНТИРОВАТЬ SECRET В ПОДЕ)
        - mountPath: /etc/secret-redis (путь монтирования, если такой папки нет то она создастся автоматически)
          name: secret-redis (имя тома который монтирует в контейнер)
          readOnly: true (политика взаимодействия)
  volumes: (определяем том)
    - name: secret-redis (имя тома который монитуем)
      secret: (определение секрета)
        secretName: my-secret (имя секрета как в манифесте)


ПРИМЕР КОНФИГУРАЦИИ Deployment 
apiVersion: apps/v1 (версия api зависит от типа службы)
kind: Deployment (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
     name: my-nginx (имя пода, оно может быть, какое захочешь)
     labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
         app: myapp (это метка, имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
       metadata: (под ним заполняем данные о службе)
          labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
             app: myapp (имя метки и ее значение может быть любым)
       spec: (это поле называется спецификация)
           affinity: (афинити создаются для привязкок подов на уровне узла)(ЭТО ТИП ВЗАИМОДЕЙСТВИЯ ТОЛЬКО НА УРОВНЕ УЗЛОВ) (ВЫБИРАТЬ ТОЛЬКО ОДИН ИЗ ПРИВЕДЕННЫХ ВАРИАНТОВ affinity)
             nodeAffinity: (для планирования на конкретном узле)(для распределения подов по конкретным узлам сначала нужно создать для этих узлов специальные метки которые потом укажем)
               requiredDuringSchedulingIgnoredDuringExecution: (ниже описываем что эта affinity обязательно должно быть выполнена при процедуре распределения)
                  nodeSelectorTerms: (под ним можно указать ряд ключей со значениями хостов на которых можно сделать размещение)
                  - matchExpressions: (совподающие выражения, ниже указываем параметры метки на которую ссылаемся)
                    - key: disktype (название ключа метки ноды на которую нужно ссылаться) (метка для узла должа быть создана предварительно командой из списка команд)
                      operator: In
                      values: (ниже указываем значение ключа ноды)
                       - ssd (его значение)
           affinity: (афинити создаются для привязкок подов на уровне узла) (ЭТО ТИПЫ ВЗАИМОДЕЙСТВИЯ НА УРОВНЕ УЗЛОВ И ПОДОВ) 
             podAntiAffinity: (планировщик не будет размещать реплики на одном узле, этот тип распределят на хосты как DaemonSet)(ИЗ ДВУХ ПРЕДСТАВЛЕННЫХ ТИПОВ НИЖЕ, МОЖНО УКАЗАТЬ ТОЛЬКО podAntiAffinity ИЛИ ДВА В ОДНОМ МАНИФЕСТЕ)
                requiredDuringSchedulingIgnoredDuringExecution: (ниже описываем что эта affinity обязательно должно быть выполнена при процедуре распределения)
                  - labelSelector: (метки селектора)
                      matchExpressions: (совподающие выражения, ниже указываем параметры метки на которую ссылаемся)
                      - key: app (название ключа метки пода на которую нужно ссылаться из шаблона пода в манифесте в котором создаем афинити)
                        operator: In 
                        values: (ниже указываем значение ключа из шаблона пода в котором создаем афинити)
                         - store (его значение, в podAntiAffinity пара ключ=значение берутся только из того манифеста в котором создается podAntiAffinity)
                    topologyKey: "kubernetes.io/hostname" (этот ключ означает что не нужно создавать больше одно экземпляра такого pod на каждом хосте, если описанный pod уже имеется на хосте то еще один такой же не будет создан)
             podAffinity: (нужен для того чтобы расположить указанный pod рядом с другими pod, имеющим описанную метку селектора) (ЭТО affinity МОЖЕТ ИДТИ ТОЛЬКО КАК ДОПОЛНЕНИЕ, ЕСЛИ ЕСТЬ ТАКАЯ НУЖДА)
                requiredDuringSchedulingIgnoredDuringExecution: (ниже описываем что эта affinity обязательно должно быть выполнена при процедуре распределения)
                  - labelSelector: (метки селектора)
                      matchExpressions: (совподающие выражения, ниже указываем параметры метки на которую ссылаемся)
                      - key: proga (название ключа метки пода на которую нужно ссылаться из шаблона пода в манифесте на других хостах, то есть расположить рядом с ними)
                        operator: In 
                        values: (ниже указываем значение ключа из шаблона пода на других хостах, то есть расположить рядом с ними)
                         - java (его значение, в podAffinity пара ключ=значение указываются только из манифеста которые развернуты на других хостах, не на мастере)
                    topologyKey: "kubernetes.io/hostname" (этот ключ означает что не нужно создавать больше одно экземпляра такого pod на каждом хосте, если описанный pod уже имеется на хосте то еще один такой же не будет создан)
           nodeSelector:(селектор нод, альтернативный способ для размещения подов, можно и через nodeAffinity)(метка для узла должа быть создана предварительно командой из списка команд)
              beta.kubernetes.io/os: linux (эта метка означает, что нужно запускать поды только на хостах с Linux ОС)
              disk: ssd (эта метка означает, что нужно запускать поды только на хостах, у которых метка SSD)(метки, которые относятся хостам могуть быть любые)
           securityContext: (параметры безопасности)
              readOnlyRootFilesystem: true (файловая система root только для чтения)
              runAsNonRoot: true (не запускать от имени root)
              runAsUser: 1000 (запустить процесс под таким UID, можешь указать любой)
              runAsGroup: 1000 (запустить процесс под таким GID, можешь указать любой)
           containers: (под ним создаем описание с заполнением полей для контейнеров)
              - name: container-nginx (имя контейнера, какое захочешь)
                image: nginx (образ, который будет скачан из репозиториев и установлен)
                imagePullPolicy: Always (политика вытягивания образов, варианты опций Always - всегда вытягивать, Never - никогда не вытягивать, IfNotPresent - если не присутствует)
                securityContext: (параметры безопасности)
                  allowPrivilegeEscalation: false (политика повышенных привелегий)
                ports: (порт приложения)
                 - containerPort: 80 (указываем порт)
                   protocol: TCP (протокол по которому будут передаваться данные)
                volumeMounts: (описываем том)
                 - name: empty-volume (имя тома должно совпадать с разделом volumes:)
                   mountPath: /empty (точка монтирования, если ее не существует то кубернетес создаст ее)
                resources: (ниже описываем то, сколько ресурсов нужно выделить для пода, и запомни, если ты запросил больше, чем есть на сервере, то деплой будет всегда в ожидании)
                  requests: (запрос на выделение ресурсов, под ним указываем какие параметры и в каком обьеме выделить)
                     memory: 3000Mi  (оперативная память)
                     cpu: 1000m  (процессор)
                  limits: (предел обьема ресурсов, который можно выделить, под ним указываем какие параметры и до какого предела ограничить)
                     memory: 5000Mi (оперативная память)
                     cpu: 5000m (процессор)
                startupProbe: (проверяет запустится ли приложение в принципе)
                 failureThreshold: 30 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 timeoutSeconds: 10 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 30 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
                readinessProbe: (проверяет готово ли приложение принимать трафик, при неудаче убирается из балансировки, исполняется постоянно)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 10 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 60 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
                livenessProbe: (контроль за состоянием приложения во время его жизни, исполняется постоянно, при наличии ошибок будет перезапущен)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна проходить каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 1 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 60 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
           restartPolicy: Always (политика перезапуска если возникают проблемы в роботе)
           priorityClassName: my-priority (указывается, чтобы определить приоритет создания подов для планировщика)
           serviceAccountName: maylo_013(в продакшене некотрые аккаунты не могут создавать службы из-за ограничений, чтобы создать то что вы хотите нужно указать тот у которого есть права)
           initContainers: (запускаются при инициализации pod и должен описываться до основных контейнеров, подготавливают окружение для работы, выполнение миграций, проверки, дождаться сервис и т.д.)
            - name: init-myservice(имя контейнера, какое захочешь)(работают как обычные контейнеры,всегда выполняются до завершения, должен успешно завершиться, чтобы запустился следующий)
              image: busybox (образ, который будет скачан из репозиториев и установлен)
              command: ['sh', '-c', 'touch /empty/init-file'] (команды, которые будут выполняются перед запуском основного контейнера) 
              volumeMounts: (описываем том) (все действия над emptyDir: будут видны и главному контейнеру)
               - name: empty-volume (имя тома должно совпадать с разделом volumes:)
                 mountPath: /empty (точка монтирования, если ее не существует то кубернетес создаст ее)
           volumes: (ниже описываем том) 
            - name: empty-volume (данный тип тома монтируется в pod для всех контейнеров и данный том будет доступен всем контейнерам вместе с его содержимым)
              emptyDir: {} (тип тома "пустой" нужен для размещения кэш файлов, сохранить данные контейнера после сбоя, обмена файлами между несколькими контейнерами в pod, он виден только в pod и контейнерах, но не на хосте)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      app: myapp (это метка, имя метки и ее значение может быть любым)
  replicas: 3 (кол-во подов, которые нужно создать)
  strategy: (стратегия обновления приложения)
    type: RollingUpdate (обновляет только по заданному кол-во по очереди, нет простоя) и Recreate (сначала удалит старые и только потом создаст новые, имеет простой)
    rollingUpdate: (ниже задаются параметры обновления, они доступны только если тип стоит rollingupdate, у recreate нет параметров, задается только тип)
       maxSurge: 2 (кол-во одновременно создаваемых подов для обновления старых)
       maxUnavailable: 0 (кол-во одновременно удаляемых подов, это число прибавляется к первому параметру и получается общее кол-во обновлений)
  
       
ПРИМЕР КОНФИГУРАЦИИ NodePort (создается для деплойментов/репликасет/подов для того, чтобы выставить их наружу по протоколу TCP/UDP по указанному порту, Service - вне манифеста представляет собой правило iptables или ipvs)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service  (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
    name: my-service (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
 type: NodePort (указываем тип службы для взаимодействия с внешним миром)
 ports:  (под ним перечисляем порты)
  - port: 80 (порт самой службы - service)
    targetPort: 80 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
    nodePort: 30004 (внешней порт, по которому будет взаимодействие с внешним миром, он развернется на самом узле, чтобы внешние приложения или юзеры могли доcтучаться до контейнера)
    protocol: TCP (протокол по которому будут передаваться данные)
 selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
      app: myapp (имя метки и ее значение может быть любым, но эту метку мы берем из манифеста, который создаст поды, в разделе меток пода)
      

КОНФИГУРАЦИЯ ClasterIP(кластер создают для внутренних сервисов таких как базы данных и т.д. если не указывать тип порта сервиса, то поумолчанию создается кластер, Service - вне манифеста представляет собой правило iptables или ipvs)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service (пишем тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: service (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
  clusterIP: 10.28.90.25 (при необходимости можно вручную прописать желаемый ip-адрес)
  ports: (под ним перечисляем порты)
      port: 80 (порт самой службы - service)
      targetPort: 80 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
      port: 443 (порт самой службы - service)
      targetPort: 8443 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
      protocol: TCP (протокол по которому будут передаваться данные)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
      name: app (имя метки и ее значение может быть любым, но эту метку мы берем из манифеста, который создаст поды, в разделе меток пода)
   
   
ПРИМЕР КОНФИГУРАЦИИ HorizontalPodAutoscaler (нужен чтобы автоматически создавать поды в кластере основываясь на метриках потребления ресурсов)
apiVersion: autoscaling/v2beta1 (версия api зависит от типа службы)
kind: HorizontalPodAutoscaler (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-auto-scaler  (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
   scaleTargetRef: (описываем тот деплоймент, который будем мониторить, версия api, kind службы и его name нужно смотреть в ямл файле самой службы)
       apiVersion: apps/v1 (версия api зависит от типа службы)
       kind: Deployment (тип службы, которую мы создаем если превышены параметры описанные ниже)
       name: my-nginx (имя службы, у которой мониторим параметры, смотреть в самом деплойменте)
   minReplicas: 2 (минимальное кол-во реплик соданных если параметры будут превышены)
   maxReplicas: 6 (максимальное кол-во реплик соданных если параметры будут превышены)
   metrics: (ниже указываем метрики и предел параметров исходя из которых будет принято решение создать дополнительные поды для распределения нагрузки)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: cpu (название ресурса, процессор)
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: memory (название ресурса, оперативная память) 
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)  


ПРИМЕР КОНФИГУРАЦИИ Job 
apiVersion: batch/v1 (версия api зависит от типа службы)
kind: Job (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: hello (имя службы)
spec: (это поле называется спецификация)
  ttlSecondsAfterFinished: 50 (указывает, через сколько секунд специальный TimeToLive контроллер должен удалить завершившийся Job вместе с подами и их логами)
  backoffLimit: 1 (количество попыток. Если указать 2, то Job дважды попробует запустить под и остановится)
  activeDeadlineSeconds: 20 (количество секунд, которое отводится всему Job на выполнение)
  completions: 1 (сколько подов должны успешно завершиться, прежде чем вся задача будет считаться сделанной)
  parallelism: 1 (кол-во подов, которые будут запущены паралельно)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
    spec: (это поле называется спецификация)
      containers: (под ним создаем описание с заполнением полей для контейнеров)
      - name: hello (имя контейнера)
        image: nginx (образ, который будет скачан из репозиториев и установлен)
        args: (под ним указываем команды, которые будут выполнятся в контейнере)
        - /bin/bash (командная оболочка)
        - -c
        - apt upgrade    
      restartPolicy: Never (политика перезапуска контейнера)


ПРИМЕР КОНФИГУРАЦИИ Secret С ОПРЕДЕЛЕНИЕМ ЛОГИНА И ПОРОЛЯ В КОДИРОВКЕ base64
apiVersion: v1 (версия api зависит от типа службы)
kind: Secret (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: my-secret (имя службы)
type: Opaque
data: (под ним указываем данные, которые будут храниться в секрете, ключь и значение могут быть любые и должны всегда быть в кодировке)
  username: YWRtaW4= (логин в кодировке base64)
  password: bWljaGE= (пароль в кодировке base64)


ПРИМЕР КОНФИГУРАЦИИ ConfigMap С ПЕРЕМЕННЫМИ СРЕДЫ
apiVersion: v1 (версия api зависит от типа службы)
kind: ConfigMap (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: apps (имя службы)
  labels: (это метка, имя метки и ее значение может быть любым)
    app: back (метка)
    label: back (метка)
data: (ниже описываем переменные и значения которые хотим задать для приложения которое после запуска прочитает их и применит их к своей конфигурации, можно определить в манифесте целиком, а не каждую отдельно) 
  Jwt__Authority: http://any/path/site 
  oidc__IssuerUrl: http://any/path/site
  oidc__ClientId: apps
  oidc__TokenUrl: http://any/path/site


ПРИМЕР КОНФИГУРАЦИИ DaemonSet
apiVersion: apps/v1 (версия api зависит от типа службы)
kind: DaemonSet (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: fluentd-elasticsearch (имя службы)
  labels: (это метка, имя метки и ее значение может быть любым)
    k8s-app: fluentd-logging (метка)
spec: (это поле называется спецификация)
  selector: (селектор помогает daemonset понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      name: fluentd-elasticsearch (это метка)
  updateStrategy: (стратегия обновления)
    type: RollingUpdate (обновляет только по заданному кол-во по очереди, нет простоя) и Recreate (сначала удалит старые и только потом создаст новые, имеет простой)
    rollingUpdate:  (ниже задаются параметры обновления, они доступны только если тип стоит rollingupdate, у recreate нет параметров, задается только тип)
      maxUnavailable: 1 (кол-во одновременно удаляемых подов)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
    metadata: (под ним заполняем данные о службе)
      labels: (это метка, имя метки и ее значение может быть любым)
        name: fluentd-elasticsearch (метка)
    spec: (это поле называется спецификация) 
      containers: (под ним создаем описание с заполнением полей для контейнеров)
      - name: fluentd-elasticsearch (имя контейнера)
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 (образ программы)
        ports: (порт приложения)
         - containerPort: 80 (указываем порт)
           protocol: TCP (протокол по которому будут передаваться данные)
        resources: (ниже описываем то, сколько ресурсов нужно выделить для пода, и запомни, если ты запросил больше, чем есть на сервере, то деплой будет всегда в ожидании)
          requests: (запрос на выделение ресурсов, под ним указываем какие параметры и в каком обьеме выделить)
            cpu: 1000m (процессор)
            memory: 1000Mi (оперативная память)
          limits: (предел обьема ресурсов, который можно выделить, под ним указываем какие параметры и до какого предела ограничить)
            memory: 1000Mi (оперативная память)
            cpu: 1000m (процессор)
        volumeMounts: (под ним описываем пути монтирования в контейнере)
        - name: root (имя тома должно совпадать с разделом volumes:)
          mountPath: /root (точка монтирования, если ее не существует то кубернетес создаст ее)
        - name: varlog (имя тома должно совпадать с разделом volumes:)
          mountPath: /var/log (точка монтирования, если ее не существует то кубернетес создаст ее)
        - name: varlibdockercontainers (имя тома должно совпадать с разделом volumes:)
          mountPath: /var/lib/docker/containers (точка монтирования, если ее не существует то кубернетес создаст ее)
          readOnly: true (политика - только для чтения)
      hostNetwork: true 
      hostPID: true
      securityContext: (параметры безопасности)
        readOnlyRootFilesystem: true (файловая система root только для чтения)
        runAsNonRoot: true (не запускать от имени root)
        runAsUser: 1000 (запустить процесс под таким UID, можешь указать любой)
        runAsGroup: 1000 (запустить процесс под таким GID, можешь указать любой)
      nodeSelector:  (селектор нод, то есть, хостового узла, как способ определения на какой ноде размещать поды, но можно и через афинити)
        beta.kubernetes.io/os: linux (эта метка означает, что нужно запускать поды только на хостах с Linux ОС)
        disk: ssd (эта метка означает, что нужно запускать поды только на хостах, у которых метка SSD)(метки, которые относятся хостам могуть быть любые)
      tolerations: (ниже описываем толерантность к размещению подов на хостах)
      - effect: NoSchedule(NoSchedule-запрещает размещать на узле, PreferNoSchedule-размещает на узле если нельзя где-то еще, NoExecute-выселяет с узла уже запущенные pod, NodeCondition-действует для узлов отвечающих заданному условию)
        operator: Exists(указываем что pod должен создаваться на хостах у которых поле taints: NoSchedule, это поле можно найти если сделать describe хоста)(поле effect: в манифесте это значение в поле taints: из вывода describe хоста) 
      terminationGracePeriodSeconds: 500 
      volumes: (перечень томов для монтирования в контейнер) (hostPath лучше использовать, когда к данным должны обращаться агенты или сервисы уровня узла)
      - name: varlog (имя тома, в котором будет путь из файловой системы хоста, который укажем ниже)
        hostPath: (нужен для монтирования папок из файловой системы хоста в контейнер для прямого доступа к папкам хоста, приложению, когда нужно предоставить доступ к файловой системе хоста. К логам приложения /var/log и т.д.)
          path: /var/log (путь из файловой системы хоста) (если монтируем папку которую создали сами то нужно задать на нее те же права что и под разделом securityContext: иначе не сможем с ней работать)
          type: Directory (тип тома)
      - name: varlibdockercontainers (имя тома, в котором будет путь из файловой системы хоста, который укажем ниже)
        hostPath: (нужен для монтирования папок из файловой системы хоста в контейнер для прямого доступа к папкам хоста, приложению, когда нужно предоставить доступ к файловой системе хоста. К логам приложения /var/log и т.д.)
          path: /var/lib/docker/containers (путь из файловой системы хоста) (если монтируем папку которую создали сами то нужно задать на нее те же права что и под разделом securityContext: иначе не сможем с ней работать)
          type: Directory (тип тома)
      - name: root (имя тома, в котором будет путь из файловой системы хоста, который укажем ниже)
        hostPath: (нужен для монтирования папок из файловой системы хоста в контейнер для прямого доступа к папкам хоста, приложению, когда нужно предоставить доступ к файловой системе хоста. К логам приложения /var/log и т.д.)
           path: / (путь из файловой системы хоста) (если монтируем папку которую создали сами то нужно задать на нее те же права что и под разделом securityContext: иначе не сможем с ней работать)
           type: Directory (тип тома) 


ПРИМЕР КОНФИГУРАЦИИ ПРОСТОГО ServiceAccount
apiVersion: v1 (версия api зависит от типа службы)
kind: ServiceAccount (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о пользователе)
  name: demo-user-demon (имя пользователя)
  namespace: default (пространство имен, в котором будет распологаться пользователь, на твое усмотрение)


ПРИМЕР КОНФИГУРАЦИИ ClusterRole и Role
apiVersion: rbac.authorization.k8s.io/v1 (версия api зависит от типа службы)
kind: ClusterRole (тип роли)
metadata: (под ним заполняем данные о роли)
  name: admin-admin (имя роли)
rules: (ниже указывам список того, к чему будет иметь доступ тот аккаунт, к которому будет привязана эта роль)
- apiGroups: ["", "extensions", "apps"] (api-группы, к которым принадлежат виды сервисов)
  resources: ["deployments", "replicasets", "pods", "services"] (сервисы, которые можно создавать)
  verbs: ["list", "create", "update", "patch", "delete"] (выражения, которые можно использовать для управления состоянием кластера)

  
ПРИМЕР КОНФИГУРАЦИИ СВЯЗКИ ClusterRoleBinding и RoleBinding
apiVersion: rbac.authorization.k8s.io/v1 (версия api зависит от типа службы)
kind: ClusterRoleBinding (тип связки для роли и сервис аккаунта)
metadata: (под ним заполняем данные о связке)
  name: admin-deploy-admin (имя связки)
subjects: (обьекты, к которым будет привязана роль, данные поля обязательны для заполнения)
- kind: ServiceAccount (тип обькта)
  name: demo-user-demon (имя обьекта, как в манифесте сервис аккаунта)
  namespace: default (пространство имен, в котором был создан обьект)
roleRef: (заполняем данные о роли, которую привяжем к аккаунту)
  apiGroup: rbac.authorization.k8s.io (api-группа, к которой принадлежит роль)
  kind: ClusterRole (тип роли)
  name: admin-admin (имя роли, как в манифесте)


ПРИМЕР КОНФИГУРАЦИИ ResourceQuota (требует указывать в манифесте лимит и реквест не больше и не меньше тех значений, которые в квоте, ресурсы выделяются на все пространство имен в целом)
apiVersion: v1 (версия api зависит от типа службы)
kind: ResourceQuota (тип службы)(нужно следить за ресурсами во время обновлений, потому что прежде чем удалить старые, будут созданы новые, и может не обновиться из-за дележки ресурсов)
metadata: (под ним заполняем данные о службе)
  name: compute-resources (имя службы)
  namespace: default (имя пространства имен, в котором будет создана квота)
spec: (спецификация)
  hard: (ниже указываем жесткие параметры на лимит, реквест и создание модулей)
    requests.cpu: 300m (минимум для выделения процесс)
    requests.memory: 300Mi (минимум для выделения оперативной памяти)
    limits.cpu: 300m (предел для выделения процесса)
    limits.memory: 300Mi (предел для выделения оперативной памяти)
    count/deployments.apps: "4" (кол-во деплоев, которое можно создавать)
    count/pods: "4" (кол-во подов, которое можно создавать)
    count/replicasets.apps: "4" (кол-во репликасетов, которое можно создавать)


ПРИМЕР КОНФИГУРАЦИИ LimitRange 
apiVersion: v1 (версия api)
kind: LimitRange (обычно используют для определения limit по умолчанию, чтобы поды в которых явно не указали лимит и реквест создавались с значениями из полей по умолчанию из этой службы)
metadata: (информация о службе)
   name: my-limitrange (имя службы)
   namespace: default
spec: (спецификация)
  limits: (ниже определяем ограничения)
  - type: Container (явно указанные параметры этого типа распростроняются на каждый модуль и реплику которой требуются ресурсы в пространстве, а не на все пространство в целом как в квоте)
    max: (лимит)
      cpu: 2 (процессор)
      memory: 1Gi (оперативная память)
    min: (реквест) (под контейнером задавать не ниже указанных параметров, иначе не создаст ничего, так как не соблюдены условия по ресурсам)
      cpu: 200m (процессор)
      memory: 500Mi  (оперативная память)
  - type: Container (тип того к чему будут применятся значения по умолчанию к каждому модулю и каждой реплике которым требуются ресурсы)
    default: (лимит по умолчанию)
      cpu: 300m (процессор)
      memory: 200Mi (оперативная память)
    defaultRequest: (реквест по умолчанию)
      cpu: 200m (процессор)
      memory: 100Mi (оперативная память)


ПРИМЕР КОНФИГУРАЦИИ PodDisruptionBudget (приложение останется доступным даже во время выселения по причине аппаратных неполадок, недоступностью сети или сбоями в ядре и т.д.)
apiVersion: policy/v1beta1 (версия api)
kind: PodDisruptionBudget (тип службы)
metadata: (информация о службе)
  name: frontend-pdb (имя службы)
spec: (спецификация)
  minAvailable: 5 (всегда должно иметь не менее пяти реплик, Kubernetes может выселить любое количество pod при условии, что пять из них остаются доступными)
 #maxUnavailable: 20%(в любой отдельный момент времени недоступными могут быть не более 20 % pod)(МОЖНО УКАЗАТЬ ТОЛЬКО ОДНО ИЗ ПРИВЕДЕННЫХ ПРАВИЛ СОХРАНЕНИЯ ПОДОВ, ЛИБО min, ЛИБО max)
  selector: (селектор помогает понять, какие поды нужно сохранять, метка в селекторе и в шаблоне должы быть одинаковые)
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      app: frontend (метка)


ПРИМЕР КОНФИГУРАЦИИ PriorityClass
apiVersion: scheduling.k8s.io/v1 (версия api)
kind: PriorityClass (тип службы)
metadata: (информация о службе)
  name: high-priority-nonpreempting (имя службы)
value: 100 (значение, по которому будет определятся приоритет планирования подов, чем выше значение, тем выше приоритет перед другими пода)
preemptionPolicy: Never(Если стоит Never поды этого приорити-класс не будут вытеснять поды, по умолчанию стоит PreemptLowerPriority, позволяет вытеснять модули с более низким приоритетом)
globalDefault: false (если значение поля стоит true, то использовать для подов без указания в них поля priorityClassName, политика будет работать глобально, если false, то указываем поле)
description: "описание - здесь описываем функционал манифеста"



ПРИМЕР КОНФИГУРАЦИИ NetworkPolicy egress (сетевые политики могут разрешать или запрещать входящий или исходящий трафик, он похожи на правила для брендмауера)
apiVersion: networking.k8s.io/v1 (версия api) (в документации можно также посмотреть прмеры с использованием ipBlock чтобы выбрать подсеть, но это уже специфично для конкретной ситуации)
kind: NetworkPolicy (тип службы) (после создания все что нужно трафик нужно пустить через веб-сервер nginx или apache)
metadata: (информация о службе)
  name: default.nginx (имя службы)
  labels: (ниже содаем метку, имя метки и ее значение, он может быть любым)
    netw: network (метка)
  namespace: default (имя пространства имен, в котором будет создана сетевая политика)
spec:  (спецификация)
  podSelector: (под селектором подов указываем метку пода к кторому нужно привязать политику) 
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      app: nginx (метка из пода в разделе шаблона)
  egress: (тип правила, исходящий трафик)
  - to: (под ним указаваем направление куда нужно отправлять трафик для пода)(ветвлений с to может быть неограниченное кол-во, для одного пода - одно to:, а для другого пода - другое to:)
    - podSelector:(под селектором подов указываем метку пода из которого будут присылать трафик)(если хотим под одним портом и to указать несколько подов создаем каждый как новое правило)
        matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
          app: redis-app (метка из пода в разделе шаблона)
      namespaceSelector: (селектор namespaceSelector ссылается на ту метку которая была создана командой предварительно перед размещение, создавать метки нужно для любого пространства)        
        matchLabels: (селектор matchLabels из namespaceSelector сопостовляет метку указанную в политуке с той что создана для пространства)           
          namespace: default (метка которую создали для пространства имен)    
    ports: (ниже указаваем порты, указываем порт dns 53 чтобы политика разрешала исходящему трафику обращаться к dns, исходящий трафик будет идти через него, хотя можно и другой)
    - protocol: UDP (протокол dns) 
      port: 53 (порт dns) (каждому to можно указать свой порт)
  policyTypes: (указываем тип политики которую применяем)
  - Egress (тип политики)



ПРИМЕР КОНФИГУРАЦИИ NetworkPolicy ingress (сетевые политики могут разрешать или запрещать входящий или исходящий трафик, он похожи на правила для брендмауера)
apiVersion: networking.k8s.io/v1 (версия api) (в документации можно также посмотреть прмеры с использованием ipBlock чтобы выбрать подсеть, но это уже специфично для конкретной ситуации)
kind: NetworkPolicy (тип службы) (после создания все что нужно трафик нужно пустить через веб-сервер nginx или apache)
metadata: (информация о службе)
  name: database.redis (имя службы)
  labels: (ниже содаем метку, имя метки и ее значение, он может быть любым
    netw: network (метка)
  namespace: default (имя пространства имен, в котором будет создана сетевая политика)
spec: (спецификация)
  podSelector: (под селектором подов указываем метку пода к кторому нужно привязать политику)
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      app: redis-app (метка из пода в разделе шаблона)
  ingress: (тип правила, входящего трафик)
  - from:(под ним указаваем откуда нужно принимать трафик для пода)(ветвлений с from может быть неограниченное кол-во, для одного пода - одно from:, а для другого пода - другое from:)
    - namespaceSelector: (селектор namespaceSelector ссылается на ту метку которая была создана командой предварительно перед размещение, создавать метки нужно для любого пространства)
        matchLabels: (селектор matchLabels из namespaceSelector сопостовляет метку указанную в политуке с той что создана для пространства)
          namespace: default (метка которую создали для пространства имен) 
      podSelector:(под селектором подов указываем метку пода из которого будут присылать трафик)(если хотим под одним портом и from указать несколько подов создаем каждый как новое правило)
        matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
          app: nginx (метка из пода в разделе шаблона)
    ports: (ниже указаваем порты, указываем порт через который будет входить трафик, указываем ssh)
      - port: 22 (порт входящего трафика может быть любым)(каждому from можно указать свой порт)
        protocol: TCP (протокол)
  policyTypes: (указываем тип политики которую применяем)
  - Ingress (тип политики)


