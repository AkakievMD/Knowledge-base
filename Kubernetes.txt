kubectl run имя --image=имя образа (после run пишем желаемое имя пода, а после Image имя образа из репозиториев)
kubectl describe pod/replicaset/deployment имя того, что хочешь посмотреть (после одного из трех пишем имя пода, описание которого хотим посмотреть)
kubectl get pods -o wide (покажет информацию о запущенных контейнерах с дополнительными полями ip и node) kubectl get pods -w (что бы посмотреть процессы с подами на ходу)
kubectl delete pods --all (удалить все поды) kubectl delete pod имя пода (удалит указанный под)
kubectl delete services --all (удалит все) kubectl delete services имя сервиса (удалит конкретный)
kubectl apply -f имя ямл файла (применит конфигурацию по имени файла, то есть, после изменений в файле перечитываем конфигурацию и приложение обновляется, можно сделать на живую не удаляя)
kubectl create -f имя ямл файла (создаст pod из ямл файла с конфигурацией)
kubectl replace -f имя ямл файла (эта команда применяется тогда, когда ты внутри файла изменил кол-во реплик и мы перечитываем ямл файл)
kubectl scale --replicas=кол-во -f имя файла (этой командой мы изменяем кол-во реплик в ямл файле не заходя в него)
kubectl get replicaset (чтобы посмотреть список созданных репликасет)  kubectl get replicasets/deployments (что бы посмотреть старый и новый набор реплик после обновления деплоймента)
kubectl delete replicaset/deployment/pod имя одного из них в ямл файле (удалит все экземпляры подов, которые созданы)
kubectl get all (покажет все репликасет, деплойменты и поды)
kubectl get deployments (чтобы посмотреть список созданных деплойментов)
kubectl rollout status deployment/имя деплоймента (чтобы посмотреть состояние выкатки обновлений на ходу)
kubectl rollout restart deployment/имя деплоймента (передеплоить текущую версию)
kubectl rollout history deployment/имя деплоймента (для просмотра списка ревизий и истории изменений)
kubectl rollout undo deployment/имя деплоймента (что бы откатиться назад к старой версии, если что-то пошло не так в новой)
kubectl edit deploy имя деплоймента --record (откроет манифест с большим количеством параметров, которые не указывались ямл файле, из которого создан деплоймент, для глобальных изменений)
minikube service имя сервиса --url (чтобы получить ссылку на ip-адрес, на котором работает приложение)
kubectl logs имя пода (покажет логи пода)

КОНФИГУРАЦИЯ ОДНОГО Pod
apiVersion: v1 (версия api зависит от типа службы)
kind: Pod (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-nginx (имя пода, оно может быть, какое захочешь)
   labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
      app: myapp (это метка, имя метки и ее значение может быть любым)
      type: fronte-end (это метка, имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  containers: (под ним создаем описание с заполнением полей для контейнеров)
    - name: container-nginx (имя контейнера, какое захочешь)
      image: nginx (образ, который будет скачан из репозиториев и установлен)
      imagePullPolicy: Always (политика вытягивания образов, варианты опций Always - всегда вытягивать, Never - никогда не вытягивать, IfNotPresent - если не присутствует)
      resources: (ниже описываем то, сколько ресурсов нужно выделить для пода)
        requests: (запрос на выделение ресурсов, под ним указываем какие параметры и в каком обьеме выделить)
          memory: 3000Mi  (оперативная память)
          cpu: 1000m  (процессор)
        limits: (предел обьема ресурсов, который можно выделить)
          memory: 5000Mi (оперативная память)
          cpu: 5000m (процессор)
      ports: (порт приложения)
        - containerPort: 80 (указываем порт)
          protocol: TCP (протокол по которому будут передаваться данные)

КОНФИГУРАЦИЯ ОДНОГО Deployment (replicaset не создается на проде хотя конфигурация таже, но возможностей меньше)
apiVersion: apps/v1 (версия api зависит от типа службы)
kind: Deployment (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
     name: my-nginx (имя пода, оно может быть, какое захочешь)
     labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
         type: fronte-end (это метка, имя метки и ее значение может быть любым)
         app: myapp (это метка, имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
       metadata: (под ним заполняем данные о службе)
          name: my-nginx (имя пода, оно может быть, какое захочешь)
          labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
             app: myapp (это метка, имя метки и ее значение может быть любым)
             type: fronte-end (это метка, имя метки и ее значение может быть любым)
       spec: (это поле называется спецификация)
           containers: (под ним создаем описание с заполнением полей для контейнеров)
              - name: container-nginx (имя контейнера, какое захочешь)
                image: nginx (образ, который будет скачан из репозиториев и установлен)
                imagePullPolicy: Always (политика вытягивания образов, варианты опций Always - всегда вытягивать, Never - никогда не вытягивать, IfNotPresent - если не присутствует)
                ports: (порт приложения)
                   - containerPort: 80 (указываем порт)
                     protocol: TCP (протокол по которому будут передаваться данные)
                resources: (ниже описываем то, сколько ресурсов нужно выделить для пода, и запомни, если ты запросил больше, чем есть на сервере, то деплой будет всегда в ожидании)
                  requests: (запрос на выделение ресурсов, под ним указываем какие параметры и в каком обьеме выделить)
                     memory: 3000Mi  (оперативная память)
                     cpu: 1000m  (процессор)
                  limits: (предел обьема ресурсов, который можно выделить, под ним указываем какие параметры и до какого предела ограничить)
                     memory: 5000Mi (оперативная память)
                     cpu: 5000m (процессор)
                startupProbe: (проверяет запустится ли приложение в принципе)
                  failureThreshold: 30 (количество допустимых провалов пробы, без удаления из балансировки)
                  httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                   path: / (путь, куда пойдет запрос, на каждом проде разный)
                   port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                  periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                  timeoutSeconds: 1 (время задержки перед пробой в секундах)
                readinessProbe: (проверяет готово ли приложение принимать трафик, при неудаче убирается из балансировки, исполняется постоянно)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 1 (время задержки перед пробой в секундах)
                livenessProbe: (контроль за состоянием приложения во время его жизни, исполняется постоянно, при наличии ошибок будет перезапущен)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна проходить каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 1 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 60 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые, чтобы слектор понимал, из какого пода брать прогу)
        matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода, но можно и из других частей манифеста, но есть и другие способы сопостовления)
            app: myapp (это метка, имя метки и ее значение может быть любым)
  replicas: 3 (кол-во подов, которые нужно создать)
  strategy: (стратегия обновления приложения)
    type: RollingUpdate (обновляет только по заданному кол-во, нет простоя) и Recreate (сначала удалит старые и только потом создаст новые, имеет простой)
    rollingUpdate: (ниже задаются параметры обновления, они доступны только если тип стоит rollingupdate, у recreate нет параметров, задается только тип)
       maxSurge: 2 (кол-во одновременно создаваемых подов для обновления старых)
       maxUnavailable: 0 (кол-во одновременно удаляемых подов, это число прибавляется к первому параметру и получается общее кол-во обновлений)
       
КОНФИГУРАЦИЯ ОДНОГО NodePort (создается для деплойментов/репликасет/подов для того, чтобы выставить их наружу по указанному порту)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service  (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
    name: my-service (имя сервиса, оно может быть, какое захочешь)
    labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
       name: my-service (это метка, имя метки и ее значение может быть любым)
       app: myapp (это метка, имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
 type: NodePort (указываем тип службы для взаимодействия с внешним миром)
 ports:  (под ним перечисляем порты)
  - port: 80 (порт самой службы)
    targetPort: 80 (порт самого пода, на котором контейнер нашего приложения слушает внешний мир)
    nodePort: 30004 (внешней порт, по которому будет взаимодействие с внешним миром, он развернется на самом узле, чтобы внешние приложения или юзеры могли доcтучаться до контейнера)
    protocol: TCP (протокол по которому будут передаваться данные)
 selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые, чтобы слектор понимал, из какого пода брать прогу)
      app: myapp (это метка, имя метки и ее значение может быть любым, но эту метку мы берем из того манифеста, который создаст поды, в разделе меток контейнера или пода)
      type: fronte-end (это метка, имя метки и ее значение может быть любым, но эту метку мы берем из того манифеста, который создаст поды, в разделе меток контейнера или пода)

КОНФИГУРАЦИЯ ОДНОГО ClasterIP (кластер создают для внутренних сервисов таких как базы данных и т.д. если не указывать тип порта сервиса, то поумолчанию создается кластер)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: red-serv (имя сервиса, оно может быть, какое захочешь)
   labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис) 
     name: red-serv (это метка, имя метки и ее значение может быть любым)
     app: app-voting (это метка, имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  ports: (под ним перечисляем порты)
    - port: 6379 (порт самой службы)
      targetPort: 6379 (порт самого пода, на котором контейнер нашего приложения слушает внешний мир)
      protocol: TCP (протокол по которому будут передаваться данные)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые, чтобы слектор понимал, из какого пода брать прогу)
      name: redis (это метка, имя метки и ее значение может быть любым, но эту метку мы берем из того манифеста, который создаст поды, в разделе меток контейнера или пода)
      app: app-voting (это метка, имя метки и ее значение может быть любым, но эту метку мы берем из того манифеста, который создаст поды, в разделе меток контейнера или пода)

КОНФИГУРАЦИЯ ОДНОГО HorizontalPodAutoscaler (нужен чтобы автоматически создавать поды в кластере основываясь на их метриках потребления ресурсов)
apiVersion: autoscaling/v2beta1 (версия api зависит от типа службы)
kind: HorizontalPodAutoscaler (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-auto-scaler  (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
   scaleTargetRef: (описываем тот деплоймент, который будем мониторить, версия api, kind службы и его name нужно смотреть в ямл файле самой службы)
       apiVersion: apps/v1 (версия api зависит от типа службы)
       kind: Deployment (тип службы, которую мы создаем если превышены параметры описанные ниже)
       name: my-nginx (имя службы, у которой мониторим параметры, смотреть в самом деплойменте)
   minReplicas: 2 (минимальное кол-во реплик соданных если параметры будут превышены)
   maxReplicas: 6 (максимальное кол-во реплик соданных если параметры будут превышены)
   metrics: (ниже указываем метрики и предел параметров исходя из которых будет принято решение создать дополнительные поды для распределения нагрузки)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: cpu (название ресурса, процессор)
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: memory (название ресурса, оперативная память) 
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)    