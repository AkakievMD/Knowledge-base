kubectl describe pod/replicaset/deployment имя того, что хочешь посмотреть (после одного из трех пишем имя пода, описание которого хотим посмотреть)
kubectl get pods -o wide (покажет информацию о запущенных контейнерах с дополнительными полями ip и node) kubectl get pods -w (что бы посмотреть процессы с подами на ходу)
kubectl delete pods --all (удалить все поды) kubectl delete pod имя пода (удалит указанный под) kubectl delete all --all (удалит все)
kubectl delete services --all (удалит все) kubectl delete services имя сервиса (удалит конкретный)
kubectl apply -f имя ямл файла (применит конфигурацию по имени файла, то есть, после изменений в файле перечитываем конфигурацию и приложение обновляется, можно сделать на живую не удаляя)
kubectl create -f имя ямл файла (создаст pod из ямл файла с конфигурацией)
kubectl replace -f имя ямл файла (эта команда применяется тогда, когда ты внутри файла изменил кол-во реплик и мы перечитываем ямл файл)
kubectl scale --replicas=кол-во -f имя файла (этой командой мы изменяем кол-во реплик в ямл файле не заходя в него)
kubectl get replicaset (чтобы посмотреть список созданных репликасет)  kubectl get replicasets/deployments (что бы посмотреть старый и новый набор реплик после обновления деплоймента)
kubectl delete replicaset/deployment/pod имя одного из них в ямл файле (удалит экземпляр одного из них, которые созданы)
kubectl get all (покажет все репликасет, деплойменты и поды)
kubectl get deployments (чтобы посмотреть список созданных деплойментов)
kubectl rollout status deployment/имя деплоймента (чтобы посмотреть состояние выкатки обновлений на ходу)
kubectl rollout restart deployment/имя деплоймента (передеплоить текущую версию)
kubectl rollout history deployment/имя деплоймента (для просмотра списка ревизий и истории изменений)
kubectl rollout undo deployment/имя деплоймента (что бы откатиться назад к старой версии, если что-то пошло не так в новой)
kubectl edit deploy имя деплоймента --record (откроет манифест с большим количеством параметров, которые не указывались ямл файле, из которого создан деплоймент, для глобальных изменений)
kubectl logs имя пода (покажет логи пода)
kubectl exec -it имя пода bash (зайдем в под с командной оболочкой bash)
kubectl get nodes (покажет ноды)
kubectl delete ds имя демонсета в манифесте (удалит daemonset)
kubectl get clusterrole/role (покажет имеющиеся кластер-роли или простые роли для пространств имен, указываем что-то одно для вывода)
kubectl get serviceaccount
kubectl get namespace
kubectl get quota -n название пространства имен
kubectl get clusterrolebinding/rolesbinding
minikube addons list (выведет список дективированных приложений для кластера)
minikube addons enable название того, что активируем (активирует указанное приложение для кластера)
minikube service имя сервиса --url (чтобы получить ссылку на ip-адрес, на котором работает приложение)

ПРИМЕР КОНФИГУРАЦИИ Pod С ПЕРЕМЕННЫМИ СРЕДЫ 
apiVersion: v1 (версия api зависит от типа службы)
kind: Pod (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-nginx (имя пода, оно может быть, какое захочешь)
   labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
      app: myapp (имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  containers: (под ним создаем описание с заполнением полей для контейнеров)
    - name: container-nginx (имя контейнера, какое захочешь)
      image: redis (образ, который будет скачан из репозиториев и установлен)
      imagePullPolicy: Always(Always-Образ скачивается при каждом запуске,Never-будет использоваться образ,который раннее скачан,IfNotPresent-загрузится и будет использоваться при запуске)
      resources: (ниже описываем то, сколько ресурсов нужно выделить для пода)
        requests: (запрос на выделение ресурсов, под ним указываем какие параметры и в каком обьеме выделить)
          memory: 3000Mi  (оперативная память)
          cpu: 1000m  (процессор)
        limits: (предел обьема ресурсов, который можно выделить)
          memory: 5000Mi (оперативная память)
          cpu: 5000m (процессор)
      ports: (порт приложения)
        - containerPort: 8080 (указываем порт)
          protocol: TCP (протокол по которому будут передаваться данные)
      env: (пример создания перененных окружения, ниже указываем переменные по типу - имя переменной и ее значение)
        - name: SERVICE_PORT 
          value: "8080"  
        - name: SERVICE_IP
          value: "172.17.0.1"
        - name: PROTOCOL
          value: "https"
      envFrom: (ПРИМЕР ТОГО, КАК ОПРЕДЕЛИТЬ ВСЕ ЗНАЧЕНИЯ ИЗ СЕКРЕТА КАК ПЕРЕМЕННЫЕ СРЕДЫ РАЗОМ, А НЕ ПО ОТДЕЛЬНОСТИ)
        - secretRef: 
             name: my-secret
      env: (ПРИМЕР ТОГО, КАК ОПРЕДЕЛЯТЬ ЧЕРЕЗ ПЕРЕМЕННУЮ СРЕДЫ ЗНАЧЕНИЯ ИЗ СЕКРЕТА ВНУТРЬ КОНТЕЙНЕРА, КАЖДЫЙ ПАРАМЕТР ОТДЕЛЬНО)
       - name: SECRET_USERNAME
           valueFrom: 
             secretKeyRef: 
                key: username
                name: my-secret
       - name: SECRET_RASSWORD
           valueFrom: 
             secretKeyRef: 
               name: my-secret
               key: password        
      volumeMounts: (ПРИМЕР ТОГО, КАК МОНТИРОВАТЬ SECRET В ПОДЕ)
        - mountPath: "/etc/secret-redis"
          name: secret-redis
          readOnly: true
  volumes:
    - name: secret-redis
      secret:
        secretName: my-secret

ПРИМЕР КОНФИГУРАЦИИ Deployment (replicaset не создается на проде хотя конфигурация таже, но возможностей меньше)
apiVersion: apps/v1 (версия api зависит от типа службы)
kind: Deployment (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
     name: my-nginx (имя пода, оно может быть, какое захочешь)
     labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
         app: myapp (это метка, имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
       metadata: (под ним заполняем данные о службе)
          name: my-nginx (имя пода, оно может быть, какое захочешь)
          labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
             app: myapp (имя метки и ее значение может быть любым)
       spec: (это поле называется спецификация)
           containers: (под ним создаем описание с заполнением полей для контейнеров)
              - name: container-nginx (имя контейнера, какое захочешь)
                image: nginx (образ, который будет скачан из репозиториев и установлен)
                imagePullPolicy: Always (политика вытягивания образов, варианты опций Always - всегда вытягивать, Never - никогда не вытягивать, IfNotPresent - если не присутствует)
                ports: (порт приложения)
                   - containerPort: 80 (указываем порт)
                     protocol: TCP (протокол по которому будут передаваться данные)
                resources: (ниже описываем то, сколько ресурсов нужно выделить для пода, и запомни, если ты запросил больше, чем есть на сервере, то деплой будет всегда в ожидании)
                  requests: (запрос на выделение ресурсов, под ним указываем какие параметры и в каком обьеме выделить)
                     memory: 3000Mi  (оперативная память)
                     cpu: 1000m  (процессор)
                  limits: (предел обьема ресурсов, который можно выделить, под ним указываем какие параметры и до какого предела ограничить)
                     memory: 5000Mi (оперативная память)
                     cpu: 5000m (процессор)
                startupProbe: (проверяет запустится ли приложение в принципе)
                  failureThreshold: 30 (количество допустимых провалов пробы, без удаления из балансировки)
                  httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                   path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                   port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                  periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                  timeoutSeconds: 1 (время задержки перед пробой в секундах)
                readinessProbe: (проверяет готово ли приложение принимать трафик, при неудаче убирается из балансировки, исполняется постоянно)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 1 (время задержки перед пробой в секундах)
                livenessProbe: (контроль за состоянием приложения во время его жизни, исполняется постоянно, при наличии ошибок будет перезапущен)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна проходить каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 1 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 60 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые, чтобы слектор понимал, из какого пода брать прогу)
        matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода, но можно и из других частей манифеста, но есть и другие способы сопостовления)
            app: myapp (это метка, имя метки и ее значение может быть любым)
  replicas: 3 (кол-во подов, которые нужно создать)
  strategy: (стратегия обновления приложения)
    type: RollingUpdate (обновляет только по заданному кол-во, нет простоя) и Recreate (сначала удалит старые и только потом создаст новые, имеет простой)
    rollingUpdate: (ниже задаются параметры обновления, они доступны только если тип стоит rollingupdate, у recreate нет параметров, задается только тип)
       maxSurge: 2 (кол-во одновременно создаваемых подов для обновления старых)
       maxUnavailable: 0 (кол-во одновременно удаляемых подов, это число прибавляется к первому параметру и получается общее кол-во обновлений)
       
ПРИМЕР КОНФИГУРАЦИИ NodePort (создается для деплойментов/репликасет/подов для того, чтобы выставить их наружу по протоколу TCP по указанному порту)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service  (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
    name: my-service (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
 type: NodePort (указываем тип службы для взаимодействия с внешним миром)
 ports:  (под ним перечисляем порты)
  - port: 80 (порт самой службы - service)
    targetPort: 80 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
    nodePort: 30004 (внешней порт, по которому будет взаимодействие с внешним миром, он развернется на самом узле, чтобы внешние приложения или юзеры могли доcтучаться до контейнера)
    protocol: TCP (протокол по которому будут передаваться данные)
 selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые, чтобы слектор понимал, из какого пода брать прогу)
      app: myapp (имя метки и ее значение может быть любым, но эту метку мы берем из манифеста, который создаст поды, в разделе меток пода)
      

КОНФИГУРАЦИЯ ClasterIP (кластер создают для внутренних сервисов таких как базы данных и т.д. если не указывать тип порта сервиса, то поумолчанию создается кластер)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service (пишем тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: service (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
  ports: (под ним перечисляем порты)
      port: 80 (порт самой службы - service)
      targetPort: 80 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
      port: 443 (порт самой службы - service)
      targetPort: 443 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
      protocol: TCP (протокол по которому будут передаваться данные)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые, чтобы слектор понимал, из какого пода брать прогу)
      name: app (имя метки и ее значение может быть любым, но эту метку мы берем из манифеста, который создаст поды, в разделе меток пода)
      
ПРИМЕР КОНФИГУРАЦИИ HorizontalPodAutoscaler (нужен чтобы автоматически создавать поды в кластере основываясь на метриках потребления ресурсов)
apiVersion: autoscaling/v2beta1 (версия api зависит от типа службы)
kind: HorizontalPodAutoscaler (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-auto-scaler  (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
   scaleTargetRef: (описываем тот деплоймент, который будем мониторить, версия api, kind службы и его name нужно смотреть в ямл файле самой службы)
       apiVersion: apps/v1 (версия api зависит от типа службы)
       kind: Deployment (тип службы, которую мы создаем если превышены параметры описанные ниже)
       name: my-nginx (имя службы, у которой мониторим параметры, смотреть в самом деплойменте)
   minReplicas: 2 (минимальное кол-во реплик соданных если параметры будут превышены)
   maxReplicas: 6 (максимальное кол-во реплик соданных если параметры будут превышены)
   metrics: (ниже указываем метрики и предел параметров исходя из которых будет принято решение создать дополнительные поды для распределения нагрузки)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: cpu (название ресурса, процессор)
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: memory (название ресурса, оперативная память) 
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)  

ПРИМЕР КОНФИГУРАЦИИ Job 
apiVersion: batch/v1 (версия api зависит от типа службы)
kind: Job (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: hello (имя службы)
spec: (это поле называется спецификация)
  ttlSecondsAfterFinished: 50 (указывает, через сколько секунд специальный TimeToLive контроллер должен удалить завершившийся Job вместе с подами и их логами)
  backoffLimit: 1 (количество попыток. Если указать 2, то Job дважды попробует запустить под и остановится)
  activeDeadlineSeconds: 20 (количество секунд, которое отводится всему Job на выполнение)
  completions: 1 (сколько подов должны успешно завершиться, прежде чем вся задача будет считаться сделанной)
  parallelism: 1 (кол-во подов, которые будут запущены паралельно)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
    spec: (это поле называется спецификация)
      containers: (под ним создаем описание с заполнением полей для контейнеров)
      - name: hello (имя контейнера)
        image: nginx (образ, который будет скачан из репозиториев и установлен)
        args: (под ним указываем команды, которые будут выполнятся в контейнере)
        - /bin/bash (командная оболочка)
        - -c
        - apt upgrade    
      restartPolicy: Never (политика перезапуска контейнера)

ПРИМЕР КОНФИГУРАЦИИ СЕКРЕТА С ОПРЕДЕЛЕНИЕМ ЛОГИНА И ПОРОЛЯ В КОДИРОВКЕ base64
apiVersion: v1
kind: Secret
metadata: 
  name: my-secret
type: Opaque
data: 
  username: YWRtaW4=
  password: bWljaGE=


ПРИМЕР КОНФИГУРАЦИИ DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        imagePullPolicy: Always
        resources:
          limits:
            memory: 1000Mi
            cpu: 1000m
          requests:
            cpu: 1000m
            memory: 1000Mi
        volumeMounts:
        - name: root
          mountPath: /root
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      hostNetwork: true
      hostPID: true
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
      nodeSelector:
        beta.kubernetes.io/os: linux
      tolerations:
      - effect: NoSchedule
        operator: Exists
      terminationGracePeriodSeconds: 500
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: root 
        hostPath: 
           path: /

ПРИМЕР КОНФИГУРАЦИИ ПРОСТОГО ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: demo-user-demon
  namespace: default


ПРИМЕР КОНФИГУРАЦИИ ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: admin-admin
rules:
- apiGroups: ["", "extensions", "apps"]
  resources: ["deployments", "replicasets", "pods", "services"]
  verbs: ["list", "create", "update", "patch", "delete"] 

 
ПРИМЕР КОНФИГУРАЦИИ СВЯЗКИ ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-deploy-admin
subjects:
- kind: ServiceAccount
  name: demo-user-demon
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: admin-admin


ПРИМЕР КОНФИГУРАЦИИ ResourceQuota С CPU И MEMORY (по сути просто заставляет в каждом манифесте в пространстве имен указывать лимит и реквест с такими же значениями как и в квоте)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    requests.cpu: 300m
    requests.memory: 300Mi
    limits.cpu: 300m
    limits.memory: 300Mi
