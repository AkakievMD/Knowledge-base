kubectl describe pod/replicaset/deployment/node имя того, что хочешь посмотреть (после одного из четырех пишем имя, описание которого хотим посмотреть)
kubectl get pods -o wide (покажет информацию о запущенных контейнерах с дополнительными полями ip и node) kubectl get pods -w (что бы посмотреть процессы с подами на ходу)
kubectl delete pod/deployment/replicaset/quota/all --all -n пространство имен (удалить все из указанного, если не указывать all и --all, то нужно указать отдельное имя того, что удаляем)
kubectl apply -f имя ямл файла (применит конфигурацию по имени файла, то есть, после изменений в файле перечитываем конфигурацию и приложение обновляется, можно сделать на живую не удаляя)
kubectl create -f имя ямл файла (создаст pod из ямл файла с конфигурацией)
kubectl replace -f имя ямл файла (эта команда применяется тогда, когда ты внутри файла изменил кол-во реплик и мы перечитываем ямл файл)
kubectl scale --replicas=кол-во -f имя файла (этой командой мы изменяем кол-во реплик в ямл файле не заходя в него)
kubectl get replicaset (чтобы посмотреть список созданных репликасет) kubectl get replicasets (что бы посмотреть старый и новый набор реплик после обновления деплоймента)
kubectl delete replicaset/deployment/pod имя одного из них в ямл файле (удалит экземпляр одного из них, которые созданы)
kubectl get all (покажет все репликасет, деплойменты и поды)  kubectl get secrets (покажет все секреты в кластере)
kubectl get deployments (чтобы посмотреть список созданных деплойментов)
kubectl rollout status deployment/имя деплоймента (чтобы посмотреть состояние выкатки обновлений на ходу)
kubectl rollout restart deployment/имя деплоймента (передеплоить текущую версию)
kubectl rollout history deployment/имя деплоймента (для просмотра списка ревизий и истории изменений)
kubectl rollout undo deployment/имя деплоймента (что бы откатиться назад к старой версии, если что-то пошло не так в новой)
kubectl edit deploy имя деплоймента --record (откроет манифест с большим количеством параметров, которые не указывались ямл файле, из которого создан деплоймент, для глобальных изменений)
kubectl logs имя пода (покажет логи пода)
kubectl exec -it имя пода bash (зайдем в под с командной оболочкой bash)
kubectl get nodes (покажет ноды) kubectl top nodes (покажет ноды в ресурсами потребления)
kubectl delete ds имя демонсета в манифесте (удалит daemonset)
kubectl get clusterrole/role (покажет имеющиеся кластер-роли или простые роли для пространств имен, указываем что-то одно для вывода)
kubectl get quota -n название пространства имен (выведет все квоты в указанном пространстве имен)
kubectl get serviceaccount (покажет все сервис аккаунты в кластере)
kubectl get namespace (покажет все пространства имен в кластере)
kubectl get clusterrolebinding/rolesbinding (выведет все связки в кластере указанного типа)
kubectl logs --f имя пода (покажет логи всего пода)
kubectl logs имя пода -с имя контейнера как в манифесте (покажет логи всего контейнера)
kubectl logs --tail=10 имя пода (выведет указанное кол-во строк лога с конца)
kubectl api-versions (покажет версии api в кластере)   kubectl api-resources (покажет api ресурсов в кластере)
minikube addons list (выведет список дективированных приложений для кластера)
minikube addons enable название того, что активируем (активирует указанное приложение для кластера)
minikube service имя сервиса --url (чтобы получить ссылку на ip-адрес, на котором работает приложение)
minikube dashboard (откроет веб-интерфейс)

ПРИМЕР КОНФИГУРАЦИИ Pod С ПЕРЕМЕННЫМИ СРЕДЫ 
apiVersion: v1 (версия api зависит от типа службы)
kind: Pod (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-nginx (имя пода, оно может быть, какое захочешь)
   labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
      app: myapp (имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  containers: (под ним создаем описание с заполнением полей для контейнеров)
    - name: container-nginx (имя контейнера, какое захочешь)
      image: redis (образ, который будет скачан из репозиториев и установлен)
      imagePullPolicy: Always(Always-Образ скачивается при каждом запуске,Never-будет использоваться образ,который раннее скачан,IfNotPresent-загрузится и будет использоваться при запуске)
      ports: (порт приложения)
        - containerPort: 8080 (указываем порт)
          protocol: TCP (протокол по которому будут передаваться данные)
      env: (пример создания перененных окружения, ниже указываем переменные по типу - имя переменной и ее значение)
        - name: SERVICE_PORT (имя перменной)
          value: "8080"  (значение)
        - name: SERVICE_IP (имя перменной)
          value: "172.17.0.1" (значение)
        - name: PROTOCOL  (имя перменной)
          value: "https" (значение)
      envFrom: (ПРИМЕР ТОГО, КАК ОПРЕДЕЛИТЬ ВСЕ ЗНАЧЕНИЯ ИЗ СЕКРЕТА КАК ПЕРЕМЕННЫЕ СРЕДЫ РАЗОМ, А НЕ ПО ОТДЕЛЬНОСТИ)
        - secretRef:  (то откуда берем)
             name: my-secret (имя секрета как в манифесте)
      env: (ПРИМЕР ТОГО, КАК ОПРЕДЕЛИТЬ ЧЕРЕЗ ПЕРЕМЕННУЮ СРЕДЫ ЗНАЧЕНИЯ ВЗЯТЫЕ ИЗ СЕКРЕТА ВНУТРЬ КОНТЕЙНЕРА, КАЖДЫЙ ПАРАМЕТР ОТДЕЛЬНО)
       - name: SECRET_USERNAME (имя перменной какое захочешь)
           valueFrom: (ниже определяем откуда берем данные)
             secretKeyRef: (то откуда берем)
                key: username (имя ключа в секрете значение которого положим в переменную) 
                name: my-secret (имя секрета как в манифесте)
       - name: SECRET_RASSWORD (имя перменной какое захочешь)
           valueFrom: (ниже определяем откуда берем данные)
             secretKeyRef: (то откуда берем)
               name: my-secret (имя секрета как в манифесте)
               key: password  (имя ключа в секрете значение которого положим в переменную)      
      volumeMounts: (ПРИМЕР ТОГО, КАК МОНТИРОВАТЬ SECRET В ПОДЕ)
        - mountPath: /etc/secret-redis (путь монтирования, если такой папки нет то она создастся автоматически)
          name: secret-redis (имя тома который монтирует в контейнер)
          readOnly: true (политика взаимодействия)
  volumes: (определяем том)
    - name: secret-redis (имя тома который монитуем)
      secret: (определение секрета)
        secretName: my-secret (имя секрета как в манифесте)



ПРИМЕР КОНФИГУРАЦИИ Deployment (replicaset не создается на проде, хотя конфигурация таже, но возможностей меньше)
apiVersion: apps/v1 (версия api зависит от типа службы)
kind: Deployment (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
     name: my-nginx (имя пода, оно может быть, какое захочешь)
     labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
         app: myapp (это метка, имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
       metadata: (под ним заполняем данные о службе)
          name: my-nginx (имя пода, оно может быть, какое захочешь)
          labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
             app: myapp (имя метки и ее значение может быть любым)
       spec: (это поле называется спецификация)
           affinity: (афинити создаются для привязкок подов на уровне узла)(ЭТО ТИП ВЗАИМОДЕЙСТВИЯ ТОЛЬКО НА УРОВНЕ УЗЛОВ) (ВЫБИРАТЬ ТОЛЬКО ОДИН ИЗ ПРИВЕДЕННЫХ ВАРИАНТОВ)
             nodeAffinity: (для планирования на конкретном узле)
               requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms: (под ним можно указать ряд ключей со значениями хостов на которых можно сделать размещение)
                  - matchExpressions: (совподающие выражения, ниже указываем параметры метки на которую ссылаемся)
                    - key: disktype (название ключа метки ноды на которую нужно ссылаться)
                      operator: In
                      values: (ниже указываем значение ключа ноды)
                       - ssd (его значение)
           affinity: (афинити создаются для привязкок подов на уровне узла) (ЭТО ТИПЫ ВЗАИМОДЕЙСТВИЯ НА УРОВНЕ УЗЛОВ И ПОДОВ)
             podAntiAffinity: (планировщик не будет размещать реплики на одном узле.)(ИЗ ДВУХ ПРЕДСТАВЛЕННЫХ ТИПОВ АФИНИТИ ВЫБЕРАЕТСЯ ТОЛЬКО ОДИН ИЛИ СОЗДАЮТСЯ ДВА В ОДНОМ МАНИФЕСТЕ)
            #podAffinity: (сообщает планировщику, что все его реплики должны быть расположены вместе с модулями, имеющими метку селектора app=store - эта метка может быть любой)
                requiredDuringSchedulingIgnoredDuringExecution: (требуется во время планирования, игнорируется во время выполнения - "такой перевод")
                  - labelSelector: (метки селектора)
                      matchExpressions: (совподающие выражения, ниже указываем параметры метки на которую ссылаемся)
                      - key: app (название ключа метки пода на которую нужно ссылаться из шаблона пода в манифесте в котором создаем афинити)
                        operator: In 
                        values: (ниже указываем значение ключа из шаблона пода в котором создаем афинити)
                         - store (его значение)
                    topologyKey: "kubernetes.io/hostname" 
           containers: (под ним создаем описание с заполнением полей для контейнеров)
              - name: container-nginx (имя контейнера, какое захочешь)
                image: nginx (образ, который будет скачан из репозиториев и установлен)
                imagePullPolicy: Always (политика вытягивания образов, варианты опций Always - всегда вытягивать, Never - никогда не вытягивать, IfNotPresent - если не присутствует)
                securityContext: (параметры безопасности)
                  allowPrivilegeEscalation: false (политика повышенных привелегий)
                ports: (порт приложения)
                   - containerPort: 80 (указываем порт)
                     protocol: TCP (протокол по которому будут передаваться данные)
                resources: (ниже описываем то, сколько ресурсов нужно выделить для пода, и запомни, если ты запросил больше, чем есть на сервере, то деплой будет всегда в ожидании)
                  requests: (запрос на выделение ресурсов, под ним указываем какие параметры и в каком обьеме выделить)
                     memory: 3000Mi  (оперативная память)
                     cpu: 1000m  (процессор)
                  limits: (предел обьема ресурсов, который можно выделить, под ним указываем какие параметры и до какого предела ограничить)
                     memory: 5000Mi (оперативная память)
                     cpu: 5000m (процессор)
           nodeSelector:  (селектор нод, то есть, хостового узла, как способ определения на какой ноде размещать поды, но можно и через афинити)
              beta.kubernetes.io/os: linux (эта метка означает, что нужно запускать поды только на хостах с Linux ОС)
              disk: ssd (эта метка означает, что нужно запускать поды только на хостах, у которых жесткий диск SSD)(метки, которые относятся хостам могуть быть любые)
           securityContext: (параметры безопасности)
              readOnlyRootFilesystem: true (файловая система root только для чтения)
              runAsNonRoot: true (не запускать от имени root)
              runAsUser: 1000 (запустить процесс под таким UID, можешь указать любой)
              runAsGroup: 1000 (запустить процесс под таким GID, можешь указать любой)
                startupProbe: (проверяет запустится ли приложение в принципе)
                 failureThreshold: 30 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 timeoutSeconds: 10 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 30 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
                readinessProbe: (проверяет готово ли приложение принимать трафик, при неудаче убирается из балансировки, исполняется постоянно)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 10 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 60 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
                livenessProbe: (контроль за состоянием приложения во время его жизни, исполняется постоянно, при наличии ошибок будет перезапущен)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна проходить каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 1 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 60 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
           restartPolicy: Always (политика перезапуска если возникают проблемы в роботе)
           serviceAccountName: maylo_013(в продакшене некотрые аккаунты не могут создавать службы из-за ограничений, чтобы создать то что вы хотите нужно указать тот у которого есть права)
           initContainers: (запускаются при инициализации пода до запуска основных контейнеров, подготавливают окружение для работы, выполнение миграций, проверки, дождаться сервис и т.д.)
            - name: init-myservice(имя контейнера, какое захочешь)(работают как обычные контейнеры,всегда выполняются до завершения, должен успешно завершиться, чтобы запустился следующий)
              image: busybox (образ, который будет скачан из репозиториев и установлен)
              command: ['sh', '-c', 'sleep 30', 'echo hallo my friend'] (команды, которые будут выполняются перед запуском основного контейнера)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
        matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
            app: myapp (это метка, имя метки и ее значение может быть любым)
  replicas: 3 (кол-во подов, которые нужно создать)
  strategy: (стратегия обновления приложения)
    type: RollingUpdate (обновляет только по заданному кол-во по очереди, нет простоя) и Recreate (сначала удалит старые и только потом создаст новые, имеет простой)
    rollingUpdate: (ниже задаются параметры обновления, они доступны только если тип стоит rollingupdate, у recreate нет параметров, задается только тип)
       maxSurge: 2 (кол-во одновременно создаваемых подов для обновления старых)
       maxUnavailable: 0 (кол-во одновременно удаляемых подов, это число прибавляется к первому параметру и получается общее кол-во обновлений)
  
       
ПРИМЕР КОНФИГУРАЦИИ NodePort (создается для деплойментов/репликасет/подов для того, чтобы выставить их наружу по протоколу TCP по указанному порту)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service  (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
    name: my-service (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
 type: NodePort (указываем тип службы для взаимодействия с внешним миром)
 ports:  (под ним перечисляем порты)
  - port: 80 (порт самой службы - service)
    targetPort: 80 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
    nodePort: 30004 (внешней порт, по которому будет взаимодействие с внешним миром, он развернется на самом узле, чтобы внешние приложения или юзеры могли доcтучаться до контейнера)
    protocol: TCP (протокол по которому будут передаваться данные)
 selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
      app: myapp (имя метки и ее значение может быть любым, но эту метку мы берем из манифеста, который создаст поды, в разделе меток пода)
      

КОНФИГУРАЦИЯ ClasterIP (кластер создают для внутренних сервисов таких как базы данных и т.д. если не указывать тип порта сервиса, то поумолчанию создается кластер)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service (пишем тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: service (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
  ports: (под ним перечисляем порты)
      port: 80 (порт самой службы - service)
      targetPort: 80 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
      port: 443 (порт самой службы - service)
      targetPort: 8443 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
      protocol: TCP (протокол по которому будут передаваться данные)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
      name: app (имя метки и ее значение может быть любым, но эту метку мы берем из манифеста, который создаст поды, в разделе меток пода)
      
ПРИМЕР КОНФИГУРАЦИИ HorizontalPodAutoscaler (нужен чтобы автоматически создавать поды в кластере основываясь на метриках потребления ресурсов)
apiVersion: autoscaling/v2beta1 (версия api зависит от типа службы)
kind: HorizontalPodAutoscaler (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-auto-scaler  (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
   scaleTargetRef: (описываем тот деплоймент, который будем мониторить, версия api, kind службы и его name нужно смотреть в ямл файле самой службы)
       apiVersion: apps/v1 (версия api зависит от типа службы)
       kind: Deployment (тип службы, которую мы создаем если превышены параметры описанные ниже)
       name: my-nginx (имя службы, у которой мониторим параметры, смотреть в самом деплойменте)
   minReplicas: 2 (минимальное кол-во реплик соданных если параметры будут превышены)
   maxReplicas: 6 (максимальное кол-во реплик соданных если параметры будут превышены)
   metrics: (ниже указываем метрики и предел параметров исходя из которых будет принято решение создать дополнительные поды для распределения нагрузки)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: cpu (название ресурса, процессор)
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: memory (название ресурса, оперативная память) 
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)  

ПРИМЕР КОНФИГУРАЦИИ Job 
apiVersion: batch/v1 (версия api зависит от типа службы)
kind: Job (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: hello (имя службы)
spec: (это поле называется спецификация)
  ttlSecondsAfterFinished: 50 (указывает, через сколько секунд специальный TimeToLive контроллер должен удалить завершившийся Job вместе с подами и их логами)
  backoffLimit: 1 (количество попыток. Если указать 2, то Job дважды попробует запустить под и остановится)
  activeDeadlineSeconds: 20 (количество секунд, которое отводится всему Job на выполнение)
  completions: 1 (сколько подов должны успешно завершиться, прежде чем вся задача будет считаться сделанной)
  parallelism: 1 (кол-во подов, которые будут запущены паралельно)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
    spec: (это поле называется спецификация)
      containers: (под ним создаем описание с заполнением полей для контейнеров)
      - name: hello (имя контейнера)
        image: nginx (образ, который будет скачан из репозиториев и установлен)
        args: (под ним указываем команды, которые будут выполнятся в контейнере)
        - /bin/bash (командная оболочка)
        - -c
        - apt upgrade    
      restartPolicy: Never (политика перезапуска контейнера)


ПРИМЕР КОНФИГУРАЦИИ СЕКРЕТА С ОПРЕДЕЛЕНИЕМ ЛОГИНА И ПОРОЛЯ В КОДИРОВКЕ base64
apiVersion: v1 (версия api зависит от типа службы)
kind: Secret (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: my-secret (имя службы)
type: Opaque
data: (под ним указываем данные, которые будут храниться в секрете, ключь и значение могут быть любые и должны всегда быть в кодировке)
  username: YWRtaW4= (логин в кодировке base64)
  password: bWljaGE= (пароль в кодировке base64)


ПРИМЕР КОНФИГУРАЦИИ DaemonSet
apiVersion: apps/v1 (версия api зависит от типа службы)
kind: DaemonSet (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: fluentd-elasticsearch (имя службы)
  labels: (это метка, имя метки и ее значение может быть любым)
    k8s-app: fluentd-logging (метка)
spec: (это поле называется спецификация)
  selector: (селектор помогает daemonset понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      name: fluentd-elasticsearch (это метка)
  updateStrategy: (стратегия обновления)
    type: RollingUpdate (обновляет только по заданному кол-во по очереди, нет простоя) и Recreate (сначала удалит старые и только потом создаст новые, имеет простой)
    rollingUpdate:  (ниже задаются параметры обновления, они доступны только если тип стоит rollingupdate, у recreate нет параметров, задается только тип)
      maxUnavailable: 1 (кол-во одновременно удаляемых подов)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
    metadata: (под ним заполняем данные о службе)
      labels: (это метка, имя метки и ее значение может быть любым)
        name: fluentd-elasticsearch (метка)
    spec: (это поле называется спецификация) 
      containers: (под ним создаем описание с заполнением полей для контейнеров)
      - name: fluentd-elasticsearch (имя контейнера)
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 (образ программы)
        resources: (ниже описываем то, сколько ресурсов нужно выделить для пода, и запомни, если ты запросил больше, чем есть на сервере, то деплой будет всегда в ожидании)
          requests: (запрос на выделение ресурсов, под ним указываем какие параметры и в каком обьеме выделить)
            cpu: 1000m (процессор)
            memory: 1000Mi (оперативная память)
          limits: (предел обьема ресурсов, который можно выделить, под ним указываем какие параметры и до какого предела ограничить)
            memory: 1000Mi (оперативная память)
            cpu: 1000m (процессор)
        volumeMounts: (под ним описываем пути монтирования в контейнере)
        - name: root (имя тома, в документации написано, что можешь назвать как угодно)
          mountPath: /root (путь - точка монтирования, его можешь выбрать сам)
        - name: varlog (имя тома, в документации написано, что можешь назвать как угодно)
          mountPath: /var/log (путь - точка монтирования, его можешь выбрать сам)
        - name: varlibdockercontainers (имя тома, в документации написано, что можешь назвать как угодно)
          mountPath: /var/lib/docker/containers (путь - точка монтирования, его можешь выбрать сам)
          readOnly: true (политика - только для чтения)
      hostNetwork: true 
      hostPID: true
      securityContext: (параметры безопасности)
        runAsNonRoot: true (не запускать от имени root)
        runAsUser: 65534 (запустить процесс под таким UID, можешь указать любой)
      nodeSelector:  (селектор нод, то есть, хостового узла, как способ определения на какой ноде размещать поды, но можно и через афинити)
        beta.kubernetes.io/os: linux (эта метка означает, что нужно запускать поды только на хостах с Linux ОС)
        disk: ssd (эта метка означает, что нужно запускать поды только на хостах, у которых жесткий диск SSD)(метки, которые относятся хостам могуть быть любые)
      tolerations: 
      - effect: NoSchedule(NoSchedule-запрещает размещать на узле, PreferNoSchedule-размещает на узле если нельзя где-то еще, NoExecute-выселяет с узла уже запущенные pod, NodeCondition-действует для узлов отвечающих заданному условию)
        operator: Exists
      terminationGracePeriodSeconds: 500 
      volumes: (перечень томов для монтирования в контейнера) (hostPath лучше использовать, когда к данным должны обращаться агенты или сервисы уровня узла)
      - name: varlog (имя тома, в котором будет путь из файловой системы хоста, который укажем ниже)
        hostPath: (используется для монтирования каталогов из файловой системы рабочего узла в модуль для прямого доступа к каталогам узла)
          path: /var/log (путь из файловой системы хоста)
      - name: varlibdockercontainers (имя тома, в котором будет путь из файловой системы хоста, который укажем ниже)
        hostPath: (используется для монтирования каталогов из файловой системы рабочего узла в модуль для прямого доступа к каталогам узла)
          path: /var/lib/docker/containers (путь из файловой системы хоста) 
      - name: root (имя тома, в котором будет путь из файловой системы хоста, который укажем ниже)
        hostPath:  (используется для монтирования каталогов из файловой системы рабочего узла в модуль для прямого доступа к каталогам узла)
           path: / (путь из файловой системы хоста)


ПРИМЕР КОНФИГУРАЦИИ ПРОСТОГО ServiceAccount
apiVersion: v1 (версия api зависит от типа службы)
kind: ServiceAccount (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о пользователе)
  name: demo-user-demon (имя пользователя)
  namespace: default (пространство имен, в котором будет распологаться пользователь, на твое усмотрение)


ПРИМЕР КОНФИГУРАЦИИ ClusterRole и Role
apiVersion: rbac.authorization.k8s.io/v1 (версия api зависит от типа службы)
kind: ClusterRole (тип роли)
metadata: (под ним заполняем данные о роли)
  name: admin-admin (имя роли)
rules: (ниже указывам список того, к чему будет иметь доступ тот аккаунт, к которому будет привязана эта роль)
- apiGroups: ["", "extensions", "apps"] (api-группы, к которым принадлежат виды сервисов, которые указываем в поле kind)
  resources: ["deployments", "replicasets", "pods", "services"] (сервисы, которые можно создавать)
  verbs: ["list", "create", "update", "patch", "delete"] (выражения, которые можно использовать вместе с kubectl для управления состоянием кластера)

  
ПРИМЕР КОНФИГУРАЦИИ СВЯЗКИ ClusterRoleBinding и RoleBinding
apiVersion: rbac.authorization.k8s.io/v1 (версия api зависит от типа службы)
kind: ClusterRoleBinding (тип связки для роли и сервис аккаунта)
metadata: (под ним заполняем данные о связке)
  name: admin-deploy-admin (имя связки)
subjects: (обьекты, к которым будет привязана роль, данные поля обязательны для заполнения)
- kind: ServiceAccount (тип обькта)
  name: demo-user-demon (имя обьекта, как в манифесте сервис аккаунта)
  namespace: default (пространство имен, в котором был создан обьект)
roleRef: (заполняем данные о роли, которую привяжем к аккаунту)
  apiGroup: rbac.authorization.k8s.io (api-группа, к которой принадлежит роль)
  kind: ClusterRole (тип роли)
  name: admin-admin (имя роли, как в манифесте)


ПРИМЕР КОНФИГУРАЦИИ ResourceQuota С CPU И MEMORY (требует указывать в манифесте лимит и реквест не больше тех значений, которые квоте, применяется ко всем манифестам в пространстве имен)
apiVersion: v1 (версия api зависит от типа службы)
kind: ResourceQuota (тип службы)(нужно следить за ресурсами во время обновлений, потому что прежде чем удалить старые, будут созданы новые, и может не обновиться из-за дележки ресурсов)
metadata: (под ним заполняем данные о службе)
  name: compute-resources (имя службы)
  namespace: default (имя пространства имен, в котором будет создана квота)
spec: (спецификация)
  hard: (ниже указываем жесткие параметры на лимит, реквест и создание модулей)
    requests.cpu: 300m (минимум для выделения процесс)
    requests.memory: 300Mi (минимум для выделения оперативной памяти)
    limits.cpu: 300m (предел для выделения процесса)
    limits.memory: 300Mi (предел для выделения оперативной памяти)
    count/deployments.apps: "4" (кол-во деплоев, которое можно создавать)
    count/pods: "4" (кол-во подов, которое можно создавать)
    count/replicasets.apps: "4" (кол-во репликасетов, которое можно создавать)


ПРИМЕР КОНФИГУРАЦИИ LimitRange 
apiVersion: v1 (версия api)
kind: LimitRange (обычно используют для определения limit поумолчанию, чтобы поды в которых явно не указали лимит и реквест создавались с значениями из полей поумолчанию из этой службы)
metadata: (информация о службе)
   name: my-limitrange
spec: (спецификация)
  limits: (ниже определяем ограничения)
  - type: Pod (параметры этого типа почти не создают, потому как эту роль передают квоте, которая устанавливает и требует, чтобы явно определяли лимит и реквест в контейнерах)
    max: (лимит)
      cpu: 2 (процессор)
      memory: 1Gi (оперативная память)
    min: (реквест)
      cpu: 200m (процессор)
      memory: 6Mi  (оперативная память)
  - type: Container (тип того к чему будут применятся значения)
    default: (лимит поумолчанию)
      cpu: 300m (процессор)
      memory: 200Mi (оперативная память)
    defaultRequest: (реквест поумолчанию)
      cpu: 200m (процессор)
      memory: 100Mi (оперативная память)


ПРИМЕР КОНФИГУРАЦИИ PodDisruptionBudget (приложение останется доступным даже во время выселения по причине аппаратных неполадок, недоступностью сети или сбоями в ядре и т.д.)
apiVersion: policy/v1beta1 (версия api)
kind: PodDisruptionBudget (тип службы)
metadata: (информация о службе)
  name: frontend-pdb (имя службы)
spec: (спецификация)
  minAvailable: 5 (всегда должно иметь не менее пяти реплик, Kubernetes может выселить любое количество pod при условии, что пять из них остаются доступными)
 #maxUnavailable: 20%(в любой отдельный момент времени недоступными могут быть не более 20 % pod)(МОЖНО УКАЗАТЬ ТОЛЬКО ОДНО ИЗ ПРИВЕДЕННЫХ ПРАВИЛ СОХРАНЕНИЯ ПОДОВ, ЛИБО min, ЛИБО max)
  selector: (селектор помогает понять, какие поды нужно сохранять, метка в селекторе и в шаблоне должы быть одинаковые)
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      app: frontend (метка)